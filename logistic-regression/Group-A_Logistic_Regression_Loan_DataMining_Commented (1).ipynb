{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7277b12",
   "metadata": {},
   "source": [
    "6  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**Goal:** Build a logistic regression model to classify loans as **Good (1)** vs **Bad (0)** using the provided dataset.\r\n",
    "\r\n",
    "**Good Loan (1):**\r\n",
    "- Current  \r\n",
    "- Fully Paid  \r\n",
    "\r\n",
    "**Bad Loan (0):**\r\n",
    "- In Grace Period  \r\n",
    "- Late (31–120 days)  \r\n",
    "- Late (16–30 days)  \r\n",
    "- Charged Off  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ccff9e",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd02bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 0) Imports (run this first)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556bfa31",
   "metadata": {},
   "source": [
    "## 1) Read / import the data using pandas\n",
    "\n",
    "This cell tries to load `loan_application_data.csv` from the same folder as the notebook.\n",
    "If your CSV is in a different location, replace `csv_path` with your full path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc2a13",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f54809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 1) Read / import the data using pandas\n",
    "\n",
    "csv_path = \"loan_application_data.csv\"  # keep CSV in same folder as this notebook\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\vanam\\Downloads\\loan_application_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    # If needed, update this path to your computer's path:\n",
    "    # csv_path = r\"C:\\Users\\YOURNAME\\Downloads\\loan_application_data.csv\"\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find 'loan_application_data.csv' in the notebook folder.\\n\"\n",
    "        \"Put the CSV in the same folder as this notebook OR update csv_path to your local file path.\"\n",
    "    )\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f9bdd",
   "metadata": {},
   "source": [
    "## 2) Describe the data\n",
    "\n",
    "We review:\n",
    "- column names\n",
    "- data types\n",
    "- summary statistics\n",
    "- unique values for the target-related column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa9e5a",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c55565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 2) Describe the data\n",
    "\n",
    "display(df.info())\n",
    "display(df.describe(include=\"all\").T.head(20))\n",
    "\n",
    "# Quick look at loan_status categories\n",
    "if \"loan_status\" in df.columns:\n",
    "    display(df[\"loan_status\"].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"Column 'loan_status' not found. Please check df.columns:\")\n",
    "    display(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abc71b",
   "metadata": {},
   "source": [
    "## 3) Review and handle missing data\n",
    "\n",
    "We check missing values and apply simple handling:\n",
    "- For numeric columns: fill missing with **median**\n",
    "- For categorical columns: fill missing with **mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae1b06",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a05131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 3) Review missing data\n",
    "\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "missing_table = pd.DataFrame({\"missing_count\": missing, \"missing_pct\": missing_pct})\n",
    "missing_table = missing_table[missing_table[\"missing_count\"] > 0]\n",
    "\n",
    "print(\"Columns with missing values:\", missing_table.shape[0])\n",
    "display(missing_table.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cafe5",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657493a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 3b) Handle missing data (median for numeric, mode for categorical)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "num_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in df_clean.columns if c not in num_cols]\n",
    "\n",
    "# Fill numeric with median\n",
    "for c in num_cols:\n",
    "    if df_clean[c].isna().any():\n",
    "        df_clean[c] = df_clean[c].fillna(df_clean[c].median())\n",
    "\n",
    "# Fill categorical with mode (most frequent)\n",
    "for c in cat_cols:\n",
    "    if df_clean[c].isna().any():\n",
    "        mode_val = df_clean[c].mode(dropna=True)\n",
    "        fill_val = mode_val.iloc[0] if len(mode_val) else \"Unknown\"\n",
    "        df_clean[c] = df_clean[c].fillna(fill_val)\n",
    "\n",
    "# Confirm\n",
    "print(\"Total missing after cleaning:\", int(df_clean.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67a40b",
   "metadata": {},
   "source": [
    "## 4) Sample the data (50%)\n",
    "\n",
    "To reduce runtime and match the assignment requirement, we randomly sample **50%** of the cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c9f23",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 4) Sample 50% of the data\n",
    "df_sample = df_clean.sample(frac=0.50, random_state=42).reset_index(drop=True)\n",
    "print(\"Sample shape:\", df_sample.shape)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869eae7d",
   "metadata": {},
   "source": [
    "## 5) Create the target variable: `Loan_Class` from `loan_status`\n",
    "\n",
    "Mapping:\n",
    "- **Good Loan (1):** Current, Fully Paid\n",
    "- **Bad Loan (0):** In Grace Period, Late (31-120 days), Late (16-30 days), Charged Off\n",
    "\n",
    "We also keep a quick frequency check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50d875",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 5) Create Loan_Class (target)\n",
    "\n",
    "good_status = {\"Current\", \"Fully Paid\"}\n",
    "bad_status = {\"In Grace Period\", \"Late (31-120 days)\", \"Late (16-30 days)\", \"Charged Off\", \"Late (16-30 days)\", \"Late (31-120 days)\"}\n",
    "\n",
    "if \"loan_status\" not in df_sample.columns:\n",
    "    raise KeyError(\"Column 'loan_status' not found. Please confirm the dataset contains 'loan_status'.\")\n",
    "\n",
    "df_sample[\"Loan_Class\"] = df_sample[\"loan_status\"].apply(lambda x: 1 if x in good_status else 0)\n",
    "\n",
    "display(df_sample[[\"loan_status\", \"Loan_Class\"]].head(10))\n",
    "display(df_sample[\"Loan_Class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c4fc5",
   "metadata": {},
   "source": [
    "## 6) Explore the data (visualizations)\n",
    "\n",
    "Required charts:\n",
    "- Loan amount distribution\n",
    "- Interest rate distribution\n",
    "- Loan Status vs Loan Amount\n",
    "- Application Type vs Loan Amount\n",
    "- Good loan vs Bad loan count plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d91599",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d870f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# Helper: pick the correct column name for loan amount / interest rate if dataset uses slightly different names\n",
    "def pick_col(candidates, columns):\n",
    "    for c in candidates:\n",
    "        if c in columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "loan_amt_col = pick_col([\"loan_amount\", \"loan_amnt\", \"loan_amt\"], df_sample.columns)\n",
    "int_rate_col = pick_col([\"interest_rate\", \"int_rate\", \"interestRate\"], df_sample.columns)\n",
    "\n",
    "print(\"Loan amount column:\", loan_amt_col)\n",
    "print(\"Interest rate column:\", int_rate_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1e7e2",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 6a) Loan amount distribution\n",
    "\n",
    "if loan_amt_col is not None:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(df_sample[loan_amt_col], bins=30)\n",
    "    plt.title(\"Loan Amount Distribution\")\n",
    "    plt.xlabel(\"Loan Amount\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not find a loan amount column. Available columns:\")\n",
    "    display(df_sample.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dcde2",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b09227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 6b) Interest rate distribution\n",
    "\n",
    "if int_rate_col is not None:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(df_sample[int_rate_col], bins=30)\n",
    "    plt.title(\"Interest Rate Distribution\")\n",
    "    plt.xlabel(\"Interest Rate\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not find an interest rate column. Available columns:\")\n",
    "    display(df_sample.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519c25b",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 6c) Loan Status vs Loan Amount (boxplot style using matplotlib)\n",
    "\n",
    "if loan_amt_col is not None:\n",
    "    # Take top N statuses to keep the chart readable\n",
    "    top_statuses = df_sample[\"loan_status\"].value_counts().head(6).index.tolist()\n",
    "    plot_df = df_sample[df_sample[\"loan_status\"].isin(top_statuses)].copy()\n",
    "\n",
    "    groups = [plot_df.loc[plot_df[\"loan_status\"] == s, loan_amt_col].values for s in top_statuses]\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.boxplot(groups, labels=top_statuses, showfliers=False)\n",
    "    plt.title(\"Loan Status vs Loan Amount (Top Statuses)\")\n",
    "    plt.xlabel(\"Loan Status\")\n",
    "    plt.ylabel(\"Loan Amount\")\n",
    "    plt.xticks(rotation=25, ha=\"right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Loan amount column not found, skipping this plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff45d7d",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109020ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 6d) Application Type vs Loan Amount\n",
    "\n",
    "if \"application_type\" in df_sample.columns and loan_amt_col is not None:\n",
    "    app_types = df_sample[\"application_type\"].value_counts().index.tolist()\n",
    "    groups = [df_sample.loc[df_sample[\"application_type\"] == a, loan_amt_col].values for a in app_types]\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.boxplot(groups, labels=app_types, showfliers=False)\n",
    "    plt.title(\"Application Type vs Loan Amount\")\n",
    "    plt.xlabel(\"Application Type\")\n",
    "    plt.ylabel(\"Loan Amount\")\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Missing 'application_type' or loan amount column, skipping this plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747ebfb",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 6e) Good loan vs Bad loan count plot\n",
    "\n",
    "counts = df_sample[\"Loan_Class\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar([\"Bad Loan (0)\", \"Good Loan (1)\"], [counts.get(0,0), counts.get(1,0)])\n",
    "plt.title(\"Good Loan vs Bad Loan Counts\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad291ddd",
   "metadata": {},
   "source": [
    "## 7) Correlation analysis / matrix\n",
    "\n",
    "Correlation applies to numeric columns. We compute a correlation matrix and display it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f3d14-017a-4be0-97f0-e219fa279a6f",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "Correlation analysis is used to measure the strength and direction of the relationship between numerical variables.  \n",
    "The correlation coefficient ranges from **-1 to +1**:\n",
    "\n",
    "- **+1** → strong positive relationship  \n",
    "- **-1** → strong negative relationship  \n",
    "- **0** → no linear relationship  \n",
    "\n",
    "In this analysis, we examine how numerical features relate to each other and to the target variable (**Loan_Class**).  \n",
    "Understanding these relationships helps identify variables that may influence loan repayment behavior and supports feature selection for the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522c0f7",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc843f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 7) Correlation matrix for numeric columns\n",
    "numeric_df = df_sample.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "corr = numeric_df.corr(numeric_only=True)\n",
    "display(corr.round(3).head(15))\n",
    "\n",
    "# Optional heatmap (simple matplotlib)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(corr, aspect=\"auto\")\n",
    "plt.title(\"Correlation Matrix (Numeric Features)\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90, fontsize=7)\n",
    "plt.yticks(range(len(corr.columns)), corr.columns, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3a948",
   "metadata": {},
   "source": [
    "## 8) Label Encoding (categorical → numeric)\n",
    "\n",
    "We label encode the required columns **if they exist** in the dataset:\n",
    "\n",
    "- emp_title\n",
    "- state\n",
    "- loan_status\n",
    "- loan_purpose\n",
    "- application_type\n",
    "- homeownership\n",
    "- verified_income\n",
    "- initial_listing_status\n",
    "- disbursement_method\n",
    "\n",
    "> Note: We will later **drop loan_status** from X to avoid target leakage, but we still encode it here to satisfy the assignment requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39251ee",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9365f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 8) Label Encoding for selected categorical columns (only if present)\n",
    "\n",
    "encode_cols = [\n",
    "    \"emp_title\",\n",
    "    \"state\",\n",
    "    \"loan_status\",\n",
    "    \"loan_purpose\",\n",
    "    \"application_type\",\n",
    "    \"homeownership\",\n",
    "    \"verified_income\",\n",
    "    \"initial_listing_status\",\n",
    "    \"disbursement_method\",\n",
    "]\n",
    "\n",
    "df_model = df_sample.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in encode_cols:\n",
    "    if col in df_model.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    else:\n",
    "        print(f\"Column not found (skipped): {col}\")\n",
    "\n",
    "print(\"Encoded columns:\", list(label_encoders.keys()))\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e66551",
   "metadata": {},
   "source": [
    "## 9) Split the data into training and testing sets\n",
    "\n",
    "- Define `y = Loan_Class`\n",
    "- Define `X = all other columns`\n",
    "- Drop `loan_status` from X to avoid target leakage (because Loan_Class came from loan_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d295f1-60f8-465e-ba8c-8c98c746a599",
   "metadata": {},
   "source": [
    "### Training and Testing Data Split\n",
    "\n",
    "To evaluate the performance of the logistic regression model, the dataset was split into training and testing sets. The training data was used to build and fit the model, while the testing data was kept separate to evaluate how well the model performs on unseen data. This helps ensure that the model does not simply memorize the data and can generalize to new loan applications.\n",
    "\n",
    "A test size of 25% was used, meaning 75% of the data was used for training and 25% for testing. Stratified sampling was applied to preserve the original distribution of good and bad loans in both sets. This approach provides a fair and reliable assessment of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53555",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e74c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 9) Train/Test split\n",
    "\n",
    "if \"Loan_Class\" not in df_model.columns:\n",
    "    raise KeyError(\"Loan_Class was not created. Please run Step 5 before continuing.\")\n",
    "\n",
    "y = df_model[\"Loan_Class\"].astype(int)\n",
    "\n",
    "# Drop target + drop original loan_status to prevent leakage\n",
    "drop_cols = [\"Loan_Class\"]\n",
    "if \"loan_status\" in df_model.columns:\n",
    "    drop_cols.append(\"loan_status\")\n",
    "\n",
    "X = df_model.drop(columns=drop_cols)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y distribution:\", y.value_counts(normalize=True).round(3))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083f2ee",
   "metadata": {},
   "source": [
    "## 10) Apply Standard Scaler\n",
    "\n",
    "Logistic regression often performs better when numeric features are scaled.\n",
    "We apply `StandardScaler` to X_train and X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f856a",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef731ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 10) Standard scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6e87e",
   "metadata": {},
   "source": [
    "## 11) Apply Logistic Regression, train the model, and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5f701",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 11) Logistic Regression model + training + predictions\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]  # probability of class 1 (Good Loan)\n",
    "\n",
    "y_pred[:10], y_proba[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a719f7a-6943-4ff4-b024-782e2d28de0e",
   "metadata": {},
   "source": [
    "###  Confusion Matrix\n",
    "A confusion matrix is used to evaluate the performance of a classification model by comparing actual loan outcomes with the model’s predictions. It shows four possible results: true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "In this model, true positives represent good loans that were correctly predicted as good, while true negatives represent bad loans correctly predicted as bad. False positives occur when a bad loan is incorrectly predicted as good, and false negatives occur when a good loan is incorrectly predicted as bad. Analyzing these values helps understand where the model performs well and where it makes mistakes, which is important in assessing loan risk.\n",
    "\n",
    "#### Confusion Matrix (TP / FP / TN / FN Explanation)\n",
    "\n",
    "The confusion matrix breaks down the model’s predictions into four categories to evaluate classification performance:\n",
    "\n",
    "- **True Positives (TP):** Good loans that were correctly predicted as good.\n",
    "\n",
    "- **True Negatives (TN):** Bad loans that were correctly predicted as bad.\n",
    "\n",
    "- **False Positives (FP):** Bad loans that were incorrectly predicted as good.\n",
    "\n",
    "- **False Negatives (FN):** Good loans that were incorrectly predicted as bad.\n",
    "\n",
    "These values help identify the types of errors made by the model. In loan risk analysis, false positives are especially important because they represent risky loans that were mistakenly classified as safe. Understanding these counts provides deeper insight into the model’s strengths and weaknesses beyond overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220b887",
   "metadata": {},
   "source": [
    "## 12) Classification report + confusion matrix (interpretation)\n",
    "\n",
    "- Confusion matrix shows **TP/FP/TN/FN** counts\n",
    "- Classification report shows **precision, recall, f1-score, accuracy**\n",
    "\n",
    "#### The classification report provides detailed performance metrics for the logistic regression model, including precision, recall, F1-score, and accuracy.\n",
    "\n",
    "- **Precision** measures how many loans predicted as good were actually good, indicating how reliable the positive predictions are.\n",
    "- **Recall measures** how many actual good loans were correctly identified by the model, showing its ability to capture positive cases.\n",
    "- **F1-score** is the balance between precision and recall, providing a single metric that reflects overall classification performance.\n",
    "- **Accuracy** represents the overall percentage of correctly classified loans.\n",
    "\n",
    "**Interpretation idea:**\n",
    "- If recall for Good Loan is high → the model catches most good loans\n",
    "- If precision for Good Loan is high → when model predicts good, it is often correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6e6b5",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae85f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 12) Confusion matrix + classification report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0 (Bad)\", \"Actual 1 (Good)\"],\n",
    "    columns=[\"Pred 0 (Bad)\", \"Pred 1 (Good)\"]\n",
    ")\n",
    "display(cm_df)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feafbab",
   "metadata": {},
   "source": [
    "***The ROC (Receiver Operating Characteristic) curve shows the trade-off between the true positive rate and the false positive rate at different classification thresholds. It helps visualize how well the model distinguishes between good and bad loans across various probability cutoffs.***\n",
    "\n",
    "***The AUC (Area Under the Curve) summarizes the ROC curve into a single value ranging from 0 to 1. A higher AUC indicates better model performance, meaning the model has a stronger ability to separate good loans from bad loans. An AUC value closer to 1 suggests good discrimination, while a value close to 0.5 indicates performance similar to random guessing.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5fa91-9abf-4e0c-9af3-817662fe9ecb",
   "metadata": {},
   "source": [
    "## 13) ROC Curve and AUC\n",
    "\n",
    "- ROC curve plots True Positive Rate vs False Positive Rate across thresholds\n",
    "- AUC summarizes performance (0.5 = random, closer to 1.0 = better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77539c1",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Detailed commented code\n",
    "# -----------------------------\n",
    "# 13) ROC curve + AUC\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"Logistic Regression (AUC={auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random (AUC=0.5)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f175f",
   "metadata": {},
   "source": [
    "### Experience Using Logistic Regression for Data Mining\n",
    "\n",
    "Using logistic regression in this assignment helped me understand how data can be used to make classification decisions in real situations. Preparing the data was an important step, especially cleaning the data, handling missing values, and converting categorical variables into numerical form. Creating the Loan_Class variable from loan status values made the problem easier to model and clearly defined what the prediction goal was.\n",
    "\n",
    "Training the model and evaluating it using the confusion matrix, classification report, and ROC–AUC curve showed how well logistic regression can separate good loans from bad loans. Overall, this experience showed that logistic regression is a simple yet effective method for data mining, especially for binary classification problems like loan risk analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96fa82-d02c-4f0a-b68e-135eff12ed72",
   "metadata": {},
   "source": [
    "I used this approach because logistic regression is a well-established supervised learning algorithm for binary classification problems. The transformation of loan status into a binary target variable allowed the model to estimate the probability of a loan being good or bad. Applying label encoding and feature scaling ensured that categorical and numerical variables were properly prepared for model training.\n",
    "\n",
    "This structured pipeline, including train–test splitting, standardization, and evaluation using confusion matrix and ROC–AUC, follows best practices in data mining and helps ensure the model’s performance is reliable, interpretable, and free from data leakage.\n",
    "More Casual Version (natural, confident, human)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99729d5",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- This cell contains Python code used in the Logistic Regression workflow.\n",
    "- Detailed comments are added inside the code to explain *what* each step does and *why* it is required.\n",
    "- This helps understand the full data mining process clearly for academic evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086b4a0-53f5-4c3f-803b-5f305e70b66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
